{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework â„–3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Machine Translation in the wild\n",
    "In the third homework you are supposed to get the best translation you can for the EN-RU translation task.\n",
    "\n",
    "Basic approach using RNNs as encoder and decoder is implemented for you. \n",
    "\n",
    "Your ultimate task is to use the techniques we've covered, e.g.\n",
    "\n",
    "* Optimization enhancements (e.g. learning rate decay)\n",
    "\n",
    "* CNN encoder (with or without positional encoding)\n",
    "\n",
    "* attention/self-attention mechanism\n",
    "\n",
    "* pretraining the language model\n",
    "\n",
    "* [Byte Pair Encoding](https://github.com/rsennrich/subword-nmt)\n",
    "\n",
    "* or just fine-tunning BERT ;)\n",
    "\n",
    "to improve the translation quality. \n",
    "\n",
    "__Please use at least three different approaches/models and compare them (translation quality/complexity/training and evaluation time).__\n",
    "\n",
    "Write down some summary on your experiments and illustrate it with convergence plots/metrics and your thoughts. Just like you would approach a real problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might need to install the libraries below. Do it in the desired environment\n",
    "# if you are working locally.\n",
    "\n",
    "# ! pip  install subword-nmt\n",
    "# ! pip install nltk\n",
    "# ! pip install torchtext\n",
    "# ! pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to YSDA NLP course team for the data\n",
    "# (who thanks tilda and deephack teams for the data in their turn)\n",
    "\n",
    "import os\n",
    "path_do_data = '../../datasets/Machine_translation_EN_RU/data.txt'\n",
    "if not os.path.exists(path_do_data):\n",
    "    print(\"Dataset not found locally. Downloading from github.\")\n",
    "    !wget https://raw.githubusercontent.com/neychev/made_nlp_course/master/datasets/Machine_translation_EN_RU/data.txt -nc\n",
    "    path_do_data = './data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "\n",
    "import torchtext\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from subword_nmt.learn_bpe import learn_bpe\n",
    "from subword_nmt.apply_bpe import BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main part\n",
    "__Here comes the preprocessing. Do not hesitate to use BPE or more complex preprocessing ;)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline tokenizer\n",
    "tokenizer_W = WordPunctTokenizer()\n",
    "def tokenize(x, tokenizer=tokenizer_W):\n",
    "    return tokenizer.tokenize(x.lower())\n",
    "\n",
    "# other tokenizer\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "def tokenize_src(text):\n",
    "    return WordPunctTokenizer().tokenize(text.lower())[::-1]\n",
    "    #return WordPunctTokenizer().tokenize(text.lower())\n",
    "\n",
    "def tokenize_trg(text):\n",
    "    #return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "    return WordPunctTokenizer().tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize=tokenize_src,\n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize=tokenize_trg,\n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "dataset = torchtext.data.TabularDataset(\n",
    "    path=path_do_data,\n",
    "    format='tsv',\n",
    "    fields=[('trg', TRG), ('src', SRC)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = dataset.split(split_ratio=[0.8, 0.15, 0.05])\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 3)\n",
    "TRG.build_vocab(train_data, min_freq = 3)\n",
    "print(f\"Unique tokens in source (ru) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are tokens from original (RU) corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.vocab.itos[::1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from target (EN) corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG.vocab.itos[::1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is example from train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vars(train_data.examples[11]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the length distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_length = map(len, [vars(x)['src'] for x in train_data.examples])\n",
    "trg_length = map(len, [vars(x)['trg'] for x in train_data.examples])\n",
    "\n",
    "print('Length distribution in Train data')\n",
    "plt.figure(figsize=[8, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"source length\")\n",
    "plt.hist(list(src_length), bins=20);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"translation length\")\n",
    "plt.hist(list(trg_length), bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_length = map(len, [vars(x)['src'] for x in test_data.examples])\n",
    "trg_length = map(len, [vars(x)['trg'] for x in test_data.examples])\n",
    "\n",
    "print('Length distribution in Test data')\n",
    "plt.figure(figsize=[8, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"source length\")\n",
    "plt.hist(list(src_length), bins=20);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"translation length\")\n",
    "plt.hist(list(trg_length), bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1,2,3],[4,5,6]])\n",
    "for word_seq in a:\n",
    "    print(word_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model side\n",
    "__Here comes simple pipeline of NMT model learning. It almost copies the week03 practice__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from routine import train_model\n",
    "\n",
    "\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "# for position encoding\n",
    "MAX_LENGTH_INPUT_SENTENCES = max(map(len, [vars(x)['src'] for x in dataset.examples]))\n",
    "MAX_LENGTH_OUTPUT_SENTENCES = max(map(len, [vars(x)['trg'] for x in dataset.examples]))\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "#BATCH_SIZE = 24 # for notebook\n",
    "show_plots=True\n",
    "n_epochs = 11\n",
    "clip = 1\n",
    "\n",
    "\n",
    "PAD_IDX = TRG.vocab.stoi['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _len_sort_key(x):\n",
    "    return len(x.src)\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device,\n",
    "    sort_key=_len_sort_key\n",
    ")\n",
    "\n",
    "for x in train_iterator:\n",
    "    break\n",
    "print(x)\n",
    "print(x.src.shape, x.trg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTMx2 LSTMx2 (Baseline model)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import nmt_lstm\n",
    "\n",
    "encoder = nmt_lstm.Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "decoder = nmt_lstm.Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "# dont forget to put the model to the right device\n",
    "model_baseline = nmt_lstm.Seq2Seq(encoder, decoder, device).to(device)\n",
    "model_baseline.apply(init_weights)\n",
    "\n",
    "print(f'The model has {count_parameters(model_baseline):,} trainable parameters')\n",
    "\n",
    "optimizer = optim.Adam(model_baseline.parameters(), lr=1e-3)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=2)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_baseline = train_model(model_baseline, {\"train\":train_iterator,\"valid\":valid_iterator}, optimizer, criterion, scheduler, n_epochs=n_epochs,  clip=clip, show_plots=show_plots, nameModel=\"nmt_lstm.pth\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "bleu(model_baseline, test_iterator, TRG.vocab, \"LSTMx2 LSTMx2 (Baseline model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTMx2 LSTMx2 with Attention (Baseline model + Attention)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import nmt_lstmAttention\n",
    "\n",
    "encoder_att = nmt_lstmAttention.Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "decoder_att = nmt_lstmAttention.Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "# dont forget to put the model to the right device\n",
    "model_att = nmt_lstmAttention.Seq2Seq(encoder_att, decoder_att, device).to(device)\n",
    "model_att.apply(init_weights)\n",
    "\n",
    "print(f'The model has {count_parameters(model_att):,} trainable parameters')\n",
    "\n",
    "optimizer_att = optim.Adam(model_att.parameters(), lr=1e-3)\n",
    "scheduler_att = ReduceLROnPlateau(optimizer_att, mode='min', factor=0.3, patience=2)\n",
    "criterion_att = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_att = train_model(model_att, {\"train\":train_iterator,\"valid\":valid_iterator}, optimizer_att, criterion_att,scheduler_att, n_epochs=n_epochs,  clip=clip, show_plots=show_plots, nameModel=\"nmt_lstmAttention.pth\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "bleu(model_att, test_iterator, TRG.vocab, \"LSTMx2 LSTMx2 with Attention (Baseline model + Attention)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTMx2 LSTMx2 with Attention and positional encoding (Baseline model + Attention + PE)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import nmt_lstmAttentionPE \n",
    "\n",
    "encoder_attpe = nmt_lstmAttentionPE.Encoder(INPUT_DIM, ENC_EMB_DIM, MAX_LENGTH_INPUT_SENTENCES, BATCH_SIZE, HID_DIM, N_LAYERS, ENC_DROPOUT, device)\n",
    "decoder_attpe = nmt_lstmAttentionPE.Decoder(OUTPUT_DIM, DEC_EMB_DIM, MAX_LENGTH_OUTPUT_SENTENCES, BATCH_SIZE, HID_DIM, N_LAYERS, DEC_DROPOUT, device)\n",
    "\n",
    "# dont forget to put the model to the right device\n",
    "model_attpe = nmt_lstmAttentionPE.Seq2Seq(encoder_attpe, decoder_attpe, device).to(device)\n",
    "model_attpe.apply(init_weights)\n",
    "\n",
    "print(f'The model has {count_parameters(model_attpe):,} trainable parameters')\n",
    "\n",
    "optimizer_attpe = optim.Adam(model_attpe.parameters(), lr=1e-3)\n",
    "scheduler_attpe = ReduceLROnPlateau(optimizer_attpe, mode='min', factor=0.3, patience=2)\n",
    "criterion_attpe = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_attpe = train_model(model_attpe, {\"train\":train_iterator,\"valid\":valid_iterator}, optimizer_attpe, criterion_attpe, scheduler_attpe, n_epochs=n_epochs,  clip=clip, show_plots=show_plots, nameModel=\"nmt_lstmAttentionPE.pth\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "bleu(model_attpe, test_iterator, TRG.vocab, \"LSTMx2 LSTMx2 with Attention and positional encoding (Baseline model + Attention + PE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN LSTMx2 (torchnlp realization. Vanilla realization: ouput from CNN = hidden_state, cell_state)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import nmt_cnn\n",
    "\n",
    "encoder_cnn = nmt_cnn.Encoder(INPUT_DIM, ENC_EMB_DIM, 3, (2, 3, 4, 5), HID_DIM*4, ENC_DROPOUT)\n",
    "decoder_cnn = nmt_cnn.Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "# dont forget to put the model to the right device\n",
    "model_cnn = nmt_cnn.Seq2Seq(encoder_cnn, decoder_cnn, device).to(device)\n",
    "model_cnn.apply(init_weights)\n",
    "\n",
    "print(f'The model has {count_parameters(model_cnn):,} trainable parameters')\n",
    "\n",
    "optimizer_cnn = optim.Adam(model_cnn.parameters(), lr=1e-3)\n",
    "scheduler_cnn  = ReduceLROnPlateau(optimizer_cnn , mode='min', factor=0.3, patience=2)\n",
    "criterion_cnn  = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_cnn= train_model(model_cnn, {\"train\":train_iterator,\"valid\":valid_iterator}, optimizer_cnn, criterion_cnn, scheduler_cnn, n_epochs=n_epochs,  clip=clip, show_plots=show_plots, nameModel=\"nmt_CNN.pth\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "bleu(model_attpe, test_iterator, TRG.vocab, \"CNN LSTMx2 (torchnlp realization. Vanilla realization: ouput from CNN = hidden_state, cell_state)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmt_transformer\n",
    "\n",
    "model_transformer = nmt_transformer.Seq2Seq(INPUT_DIM, OUTPUT_DIM, ENC_EMB_DIM, ENC_DROPOUT, device).to(device)\n",
    "model_transformer.apply(init_weights)\n",
    "\n",
    "print(f'The model has {count_parameters(model_transformer):,} trainable parameters')\n",
    "\n",
    "optimizer_transformer = optim.Adam(model_transformer.parameters(), lr=1e-3)\n",
    "scheduler_transformer  = ReduceLROnPlateau(optimizer_transformer , mode='min', factor=0.3, patience=2)\n",
    "criterion_transformer  = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transformer= train_model(model_transformer, {\"train\":train_iterator,\"valid\":valid_iterator}, optimizer_transformer, criterion_transformer, scheduler_transformer, n_epochs=n_epochs,  clip=clip, show_plots=show_plots, nameModel=\"nmt_transformer.pth\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "bleu(model_transformer, test_iterator, TRG.vocab, \"nn.Transformer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load model (probability - for correct work need save and datasets)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(\"4.306459631238665loss_nmt_lstmAttentionPE.pth_epoch8.pth\", \"rb\") as fp:\n",
    "    best_state_dict = torch.load(fp, map_location=\"cpu\")\n",
    "\n",
    "# because of rand split:train/valid/test, the different len(SRC.vocab)/len(TRG.vocab) obtain\n",
    "#INPUT_DIM = best_state_dict[\"encoder.embedding.weight\"].shape[0]\n",
    "#OUTPUT_DIM = best_state_dict[\"decoder.out.weight\"].shape[0]\n",
    "\n",
    "#Encoder = nmt_lstm.Encoder\n",
    "#Decoder = nmt_lstm.Decoder\n",
    "#Seq2Seq = nmt_lstm.Seq2Seq\n",
    "#enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "#dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "#model_load1 = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "encoder_attpe = nmt_lstmAttentionPE.Encoder(INPUT_DIM, ENC_EMB_DIM, MAX_LENGTH_INPUT_SENTENCES, BATCH_SIZE, HID_DIM, N_LAYERS, ENC_DROPOUT, device)\n",
    "decoder_attpe = nmt_lstmAttentionPE.Decoder(OUTPUT_DIM, DEC_EMB_DIM, MAX_LENGTH_OUTPUT_SENTENCES, BATCH_SIZE, HID_DIM, N_LAYERS, DEC_DROPOUT, device)\n",
    "model_epoch8 = nmt_lstmAttentionPE.Seq2Seq(encoder_attpe, decoder_attpe, device).to(device)\n",
    "model_epoch8.load_state_dict(best_state_dict) \n",
    "\n",
    "bleu(model_load, test_iterator, TRG.vocab, \"load model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline solution BLEU score is quite low. Try to achieve at least __18__ BLEU on the test set. \n",
    "The checkpoints are:\n",
    "\n",
    "* __18__ - minimal score to submit the homework, 30% of points\n",
    "\n",
    "* __20__ - good score, 70% of points\n",
    "\n",
    "* __25__ - excellent score, 100% of points"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "homework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
